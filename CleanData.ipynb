{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "import requests\n",
    "import pandas as pd\n",
    "import base64\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk import wordnet\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv('7DayTwitterPull.csv')#, lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11184 entries, 0 to 11183\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   created_at  11184 non-null  object\n",
      " 1   id          11184 non-null  int64 \n",
      " 2   lang        11184 non-null  object\n",
      " 3   text        11184 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 349.6+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#View column quality\n",
    "data_raw.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>ID</th>\n",
       "      <th>lang</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Original Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-03T19:20:59.000Z</td>\n",
       "      <td>1345812421676576768</td>\n",
       "      <td>en</td>\n",
       "      <td>@MKBHD Step 2: buy $ETH sub $1k\\n\\nStep 3: out...</td>\n",
       "      <td>@MKBHD Step 2: buy $ETH sub $1k\\n\\nStep 3: out...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-03T19:20:59.000Z</td>\n",
       "      <td>1345812421600940032</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @Naturalmed777: I think $XRP $XLM are bigge...</td>\n",
       "      <td>RT @Naturalmed777: I think $XRP $XLM are bigge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-03T19:20:59.000Z</td>\n",
       "      <td>1345812421584285698</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @Sheresed69: New toy üòçüòç subscribe to my onl...</td>\n",
       "      <td>RT @Sheresed69: New toy üòçüòç subscribe to my onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-03T19:20:59.000Z</td>\n",
       "      <td>1345812420955144197</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @TokenGoodGuy0: .\\n#Digibyte $dgb likely to...</td>\n",
       "      <td>RT @TokenGoodGuy0: .\\n#Digibyte $dgb likely to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-03T19:20:59.000Z</td>\n",
       "      <td>1345812420783206401</td>\n",
       "      <td>en</td>\n",
       "      <td>RT @nixtetic: I use my comp. for remote and co...</td>\n",
       "      <td>RT @nixtetic: I use my comp. for remote and co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date                   ID lang  \\\n",
       "0  2021-01-03T19:20:59.000Z  1345812421676576768   en   \n",
       "1  2021-01-03T19:20:59.000Z  1345812421600940032   en   \n",
       "2  2021-01-03T19:20:59.000Z  1345812421584285698   en   \n",
       "3  2021-01-03T19:20:59.000Z  1345812420955144197   en   \n",
       "4  2021-01-03T19:20:59.000Z  1345812420783206401   en   \n",
       "\n",
       "                                               Tweet  \\\n",
       "0  @MKBHD Step 2: buy $ETH sub $1k\\n\\nStep 3: out...   \n",
       "1  RT @Naturalmed777: I think $XRP $XLM are bigge...   \n",
       "2  RT @Sheresed69: New toy üòçüòç subscribe to my onl...   \n",
       "3  RT @TokenGoodGuy0: .\\n#Digibyte $dgb likely to...   \n",
       "4  RT @nixtetic: I use my comp. for remote and co...   \n",
       "\n",
       "                                      Original Tweet  \n",
       "0  @MKBHD Step 2: buy $ETH sub $1k\\n\\nStep 3: out...  \n",
       "1  RT @Naturalmed777: I think $XRP $XLM are bigge...  \n",
       "2  RT @Sheresed69: New toy üòçüòç subscribe to my onl...  \n",
       "3  RT @TokenGoodGuy0: .\\n#Digibyte $dgb likely to...  \n",
       "4  RT @nixtetic: I use my comp. for remote and co...  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Drop duplicates\n",
    "data_raw.drop_duplicates(subset='text', keep='first', inplace=True)\n",
    "#Rename columns\n",
    "data = data_raw.rename(columns={'created_at':'Date', 'id':'ID', 'text': 'Tweet'})\n",
    "#Create Original Text column for reference\n",
    "data['Original Tweet'] = data['Tweet']\n",
    "#Preview dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    no_punct = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     MKBHD Step 2 buy ETH sub 1k\\n\\nStep 3 outperfo...\n",
       "1     RT Naturalmed777 I think XRP XLM are bigger th...\n",
       "2     RT Sheresed69 New toy üòçüòç subscribe to my onlyf...\n",
       "3     RT TokenGoodGuy0 \\nDigibyte dgb likely to move...\n",
       "4     RT nixtetic I use my comp for remote and commi...\n",
       "5     RT valentinebxxx NEW YEAR SW PROMO THREADüéâ\\n\\n...\n",
       "6     RT designernayima2 üòçAmazing Company logo for y...\n",
       "10    RT ReySantoscrypto Link to use dapp browser on...\n",
       "11    Ajaz86 PeterMcCormack Exactly I don‚Äôt want bit...\n",
       "12    amitkejriwal01 cryptokanoon Thats the point yo...\n",
       "13    RT MKBHD Step 1 Resist the urge to buy Bitcoin...\n",
       "14    Suhail Wrong eth has unlimited supply cant hol...\n",
       "15    My family‚Äôs Christmas was somewhat ruined Does...\n",
       "16    RT NFL 84 yards to the house\\n\\nIsaiah McKenzi...\n",
       "17    RT DrAmyKellam toadmeister I checked your link...\n",
       "18    Wanna know how I got to top 16 on onlyf4ns in ...\n",
       "20    Our latest work on Lymphadenopathy in Fungatin...\n",
       "21    RT Mareq16 New Parler from Dr Stella Emmanuel ...\n",
       "22    RT jennycohn1 ‚ÄúRequiring an ‚Äòaudit‚Äô would requ...\n",
       "23    As the CMEfutures wakes up tomorrow what are y...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize teach tweet\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: remove_punctuation(x))\n",
    "data['Tweet'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [mkbhd, step, 2, buy, eth, sub, 1k, step, 3, o...\n",
       "1     [rt, naturalmed777, i, think, xrp, xlm, are, b...\n",
       "2     [rt, sheresed69, new, toy, subscribe, to, my, ...\n",
       "3     [rt, tokengoodguy0, digibyte, dgb, likely, to,...\n",
       "4     [rt, nixtetic, i, use, my, comp, for, remote, ...\n",
       "5     [rt, valentinebxxx, new, year, sw, promo, thre...\n",
       "6     [rt, designernayima2, amazing, company, logo, ...\n",
       "10    [rt, reysantoscrypto, link, to, use, dapp, bro...\n",
       "11    [ajaz86, petermccormack, exactly, i, don, t, w...\n",
       "12    [amitkejriwal01, cryptokanoon, thats, the, poi...\n",
       "13    [rt, mkbhd, step, 1, resist, the, urge, to, bu...\n",
       "14    [suhail, wrong, eth, has, unlimited, supply, c...\n",
       "15    [my, family, s, christmas, was, somewhat, ruin...\n",
       "16    [rt, nfl, 84, yards, to, the, house, isaiah, m...\n",
       "17    [rt, dramykellam, toadmeister, i, checked, you...\n",
       "18    [wanna, know, how, i, got, to, top, 16, on, on...\n",
       "20    [our, latest, work, on, lymphadenopathy, in, f...\n",
       "21    [rt, mareq16, new, parler, from, dr, stella, e...\n",
       "22    [rt, jennycohn1, requiring, an, audit, would, ...\n",
       "23    [as, the, cmefutures, wakes, up, tomorrow, wha...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenize teach tweet\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: tokenizer.tokenize(x.lower()))\n",
    "data['Tweet'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stop words such as \"i\", \"me\", \"my\", \"you\" since they low predictive power (do not want to do with very small samples)\n",
    "def remove_stopwords(text):\n",
    "    words = [w for w in text if w not in stopwords.words('english')]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [mkbhd, step, 2, buy, eth, sub, 1k, step, 3, o...\n",
       "1    [rt, naturalmed777, think, xrp, xlm, bigger, p...\n",
       "2    [rt, sheresed69, new, toy, subscribe, onlyfans...\n",
       "3    [rt, tokengoodguy0, digibyte, dgb, likely, mov...\n",
       "4    [rt, nixtetic, use, comp, remote, commission, ...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Tweet'] = data['Tweet'].apply(lambda x : remove_stopwords(x))\n",
    "data['Tweet'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Stemming & Lemmatizing* - both \"stem\" a word down to it's base\n",
    "* Stemming - Faster, truncates the word and is less acccuurate\n",
    "* Lemmatizing - Slower, finds the original word instead of truncating - more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#Trying Stemmer (add \"\".join to the front because we will use this module)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def word_lemmatizer(text):\n",
    "    lem_text = \" \".join([lemmatizer.lemmatize(i) for i in text])\n",
    "    return lem_text\n",
    "def word_stemmer(text):\n",
    "    stem_text = \" \".join([stemmer.stem(i) for i in text])\n",
    "    return stem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using word_stemmer\n",
    "#data['text'] = data['text'].apply(lambda x: word_stemmer(x))\n",
    "#data['text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    mkbhd step 2 buy eth sub 1k step 3 outperform ...\n",
       "1    rt naturalmed777 think xrp xlm bigger people r...\n",
       "2    rt sheresed69 new toy subscribe onlyfans see s...\n",
       "3    rt tokengoodguy0 digibyte dgb likely move fast...\n",
       "4    rt nixtetic use comp remote commission work ha...\n",
       "Name: Tweet, dtype: object"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using word_lemmatizer\n",
    "data['Tweet'] = data['Tweet'].apply(lambda x: word_lemmatizer(x))\n",
    "data['Tweet'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save cleaned dataframe to csv for analysis\n",
    "data.to_csv('Dec28toJan03_Clean.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
